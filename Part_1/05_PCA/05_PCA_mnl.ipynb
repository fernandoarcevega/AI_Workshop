{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOYiL9K+WZ+UxFQglxcHxQO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<p align=\"left\">\n","  <a href=\"https://colab.research.google.com/github/fernandoarcevega/AI_Workshop/blob/main/Part_1/05_PCA/05_PCA_mnl.ipynb\" target=\"_parent\">\n","    <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\" width=\"200\">\n","  </a>\n","</p>"],"metadata":{"id":"ucdxhcyKl08J"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ii81IR_gDo8"},"outputs":[],"source":["###############################################\n","# Author 1: Wilfrido G칩mez-Flores (CINVESTAV) #\n","# Author 2: Fernando Arce-Vega (CIO)          #\n","# e-mail 1: wilfrido.gomez@cinvestav.mx       #\n","# e-mail 2: farce@cio.mx                      #\n","# Date:     nov/03/2025                       #\n","# Subject:  Principal component analysis      #\n","###############################################"]},{"cell_type":"code","source":["# Libraries\n","import numpy as np                       # Numerical array operations\n","import pandas as pd                      # Data manipulation/analysis\n","import matplotlib.pyplot as plt          # Data plotting/visualization"],"metadata":{"id":"4H_6DdrPgb_L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Other functions\n","!wget -q https://raw.githubusercontent.com/fernandoarcevega/AI_Workshop/main/helpers/misc.py\n","from misc import *"],"metadata":{"id":"Cona5I1Gl5SA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load dataset\n","# Path to dataset\n","path = 'https://raw.githubusercontent.com/fernandoarcevega/AI_Workshop/main/datasets/bus_data.csv'\n","T = pd.read_csv(path)\n","data = T.values\n","n, d = data.shape\n","X = data[:, :d-1]  # Features\n","Y = data[:, d-1]   # Class labels"],"metadata":{"id":"6bMUrc4Igc4f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check feature and targets shapes\n","print(f'Features shape: {X.shape}')\n","print(f'Targets shape:  {Y.shape}')"],"metadata":{"id":"f5-IgHcghz0u","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761953232168,"user_tz":360,"elapsed":56,"user":{"displayName":"Fernando Arce","userId":"09517358891541310336"}},"outputId":"f4c6053b-7a19-4372-f5aa-a8b880c0590b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Features shape: (3287, 19)\n","Targets shape:  (3287,)\n"]}]},{"cell_type":"markdown","source":[" 游댯 游댯 游댯 游댯 游댯 游댯 游댯 游댯 游댯 游댯  (Total data)\n","\n"," 游릭 游릭 游릭 游릭 游릭 游릭 游릭 游릭 游리 游리  (Training and testing data)"],"metadata":{"id":"Grb8lwBIh3Qi"}},{"cell_type":"code","source":["# Split dataset into training and test sets (80%-20%)\n","tr, tt = HoldOut(Y, 0.2)\n","\n","# Training set\n","Xtr = X[tr, :]\n","Ytr = Y[tr]\n","\n","# Test set\n","Xtt = X[tt, :]\n","Ytt = Y[tt]"],"metadata":{"id":"L_Nd55YBh0SI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check feature and targets shapes for training and test sets\n","print(f'Training features shape: {Xtr.shape}')\n","print(f'Training targets shape:  {Ytr.shape} \\n')\n","print(f'Testing features shape:  {Xtt.shape}')\n","print(f'Testing targets shape:   {Ytt.shape}')"],"metadata":{"id":"2qFKshnyjmef","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761953232239,"user_tz":360,"elapsed":17,"user":{"displayName":"Fernando Arce","userId":"09517358891541310336"}},"outputId":"89ddb6b0-d284-46ab-c1ab-6ebd43e8e7fa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training features shape: (2630, 19)\n","Training targets shape:  (2630,) \n","\n","Testing features shape:  (657, 19)\n","Testing targets shape:   (657,)\n"]}]},{"cell_type":"code","source":["# Data normalization\n","Xtr, stats = zscorenorm(Xtr)\n","Xtt = zscorenorm(Xtt, stats)"],"metadata":{"id":"UC7c4935h_EV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Algorithm: Principal Component Analysis (PCA) Steps\n","\n","1.  Centralize data to have a zero mean\n","2.  Compute the covariance matrix\n","3.  Obtain eigenvalues and eigenvectors\n","4.  Sort eigenvalues in descending order\n","5.  Reorder eigenvectors accordingly\n","6.  Pick the first $q$ sorted eigenvectors\n","7.  Project the original data onto the selected eigenvectors (i.e., the largest PCS)\n"],"metadata":{"id":"-j_AejsryunT"}},{"cell_type":"code","source":["# PCA\n","def my_pca(X):\n","    n = X.shape[0]                                        # Number of samples\n","    mn = np.mean(X, axis=0)                               # Mean of the columns\n","    X_centered = X - mn                                   # Centralize data to have a zero mean\n","    S = np.dot(X_centered.T, X_centered) / n              # Compute the covariance matrix\n","    eigenvalues, eigenvectors = np.linalg.eig(S)          # Obtain eigenvalues and eigenvectors\n","    eigenvectors = np.real_if_close(eigenvectors, tol=1)  # Real components\n","    eigenvalues = np.real_if_close(eigenvalues, tol=1)    # Real components\n","    idx = eigenvalues.argsort()[::-1]                     # Sort eigenvalues in descending order\n","    eigenvalues = eigenvalues[idx]\n","    eigenvectors = eigenvectors[:, idx]                   # Reorder eigenvectors accordingly\n","    R = 1 - eigenvalues / np.sum(eigenvalues)             # Explained variance\n","    q = np.argmax(R > 0.95) + 1                           # Top q PCs\n","    W = eigenvectors[:, :q]                               # Pick the first q sorted eigenvectors\n","    return W, q"],"metadata":{"id":"5JPg_0fXiFS3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main logistic regression\n","def LRtrain(X, Y, eta, tmax):\n","    c = np.unique(Y)\n","    nc = len(c)\n","    n = X.shape[0]\n","    X = np.hstack((np.ones((n, 1)), X))     # Augments for bias\n","\n","    if nc > 2:\n","        T = np.zeros((n, nc))\n","\n","        for i in range(nc):\n","            T[Y == c[i], i] = 1             # One-hot encoding\n","\n","        W, L = trainMclass(X, T, eta, tmax) # Softmax regression\n","\n","    else:\n","        W, L = train2class(X, Y, eta, tmax) # Logistic regression\n","\n","    return W, L"],"metadata":{"id":"inXW2Hj3lCoL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train two-class logistic regression\n","def train2class(X, T, eta, tmax):\n","    d = X.shape[1]                    # Number of features\n","    W = np.zeros(d)                   # Weight vector\n","    loss = np.zeros(tmax)\n","\n","    for t in range(tmax):             # Gradient descent loop\n","        P = 1 / (1 + np.exp(-X @ W))  # Sigmoid function\n","        W -= eta * (P - T) @ X        # Update weights\n","\n","        # Binary cross-entropy loss\n","        loss[t] = -np.mean(T * np.log(P) + (1 - T) * np.log(1 - P))\n","\n","    return W, loss"],"metadata":{"id":"_tb4MYHLwA1a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train multiclass logistic regression\n","def trainMclass(X, T, eta, tmax):\n","    d = X.shape[1]        # Number of features\n","    c = T.shape[1]        # Number of classes\n","    W = np.zeros((c, d))  # Weight matrix\n","    loss = np.zeros(tmax)\n","\n","    # Gradient descent loop\n","    for t in range(tmax):\n","        E = np.exp(X @ W.T)\n","        P = E / np.sum(E, axis=1, keepdims=True)  # Softmax\n","        W -= eta * (P - T).T @ X                  # Update weights\n","\n","        # Categorical cross-entropy loss\n","        loss[t] = -np.mean(T * np.log(P))\n","    return W, loss"],"metadata":{"id":"93M4-IQmwEnV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Predict with the trained linear model\n","def LRpredict(X, W):\n","    n = X.shape[0]\n","    X = np.hstack((np.ones((n, 1)), X))   # Augmented\n","\n","    if W.ndim > 1:\n","        E = np.exp(X @ W.T)\n","        Pr = E / np.sum(E, axis=1, keepdims=True) # Softmax\n","        Ypp = np.argmax(Pr, axis=1)               # Classify\n","        Ypp = Ypp+1\n","\n","    else:\n","        Pr = 1 / (1 + np.exp(-X @ W))   # Sigmoid\n","        Ypp = (Pr > 0.5).astype(float)  # Classify\n","\n","    return Ypp, Pr"],"metadata":{"id":"f7lOjdoKwGzN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# PCA\n","Wpca, q = my_pca(Xtr)\n","Xtr2 = np.dot(Xtr, Wpca)\n","Xtt2 = np.dot(Xtt, Wpca)"],"metadata":{"id":"RqC2pFVNwL9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Principal components\n","print(f'Number of principal compoents in 95% of variance: {np.shape(Wpca)}')"],"metadata":{"id":"hnWqsJMgVYhe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761953232649,"user_tz":360,"elapsed":7,"user":{"displayName":"Fernando Arce","userId":"09517358891541310336"}},"outputId":"62247729-cc7d-4323-8b38-ae145784220b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of principal compoents in 95% of variance: (19, 7)\n"]}]},{"cell_type":"code","source":["# Without PCA\n","# Train model\n","eta = 1e-4\n","tmax = 1000\n","W, _ = LRtrain(Xtr, Ytr, eta, tmax)"],"metadata":{"id":"3ijDKIf7wMgG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Classify test data\n","Ypp, _ = LRpredict(Xtt, W)\n","err = np.mean(Ypp != Ytt)\n","print(f'Error without PCA: {100 * err:.3f}%')"],"metadata":{"id":"db48nlm7wQNI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761953232900,"user_tz":360,"elapsed":33,"user":{"displayName":"Fernando Arce","userId":"09517358891541310336"}},"outputId":"dd633443-5afd-436d-f21d-f987a8677eb8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error without PCA: 15.677%\n"]}]},{"cell_type":"code","source":["# With PCA\n","# Train model\n","W, _ = LRtrain(Xtr2, Ytr, eta, tmax)\n","\n","# Classify test data\n","Ypp, _ = LRpredict(Xtt2, W)\n","err = np.mean(Ypp != Ytt)\n","print(f'Error with {q} PCs: {100 * err:.3f}%')"],"metadata":{"id":"uz51gFGEwXeU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761953233233,"user_tz":360,"elapsed":329,"user":{"displayName":"Fernando Arce","userId":"09517358891541310336"}},"outputId":"e078c0b1-4370-4314-f7be-7096d8d50774"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Error with 7 PCs: 15.982%\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"HYNab9nsxk_7"},"execution_count":null,"outputs":[]}]}